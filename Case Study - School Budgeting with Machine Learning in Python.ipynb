{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School Budgeting with Machine Learning in Python \n",
    "\n",
    "Data science isn't just for predicting ad-clicks. It's also useful for social impact! This is a case study from a machine learning competition on DrivenData. I'll explore a problem related to school district budgeting. By building a model to automatically classify items in a school's budget, it makes it easier and faster for schools to compare their spending with other schools. I'll begin by building a baseline model that is a simple, first-pass approach. In particular, I'll do some natural language processing to prepare the budgets for modeling. Next, I'll have the opportunity to try more techniques and see how they compare to participants from the competition. Finally, I'll see how the winner who submitted his model at Driven Data was able to combine a number of expert techniques to build the most accurate model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you accurately classify line-items in a school budget based on what that money is being used for? You will explore the raw text and numeric values in the dataset, both quantitatively and visually. I will also measure success when trying to predict class labels for each row of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Introducing the challenge\n",
    "\n",
    "School budgets in the United States are incredibly complex and there are no standard s for reporting how money is spent. Schools want to be able to measure their performance, for example, are we spending more on textbooks than our neighboring schools and is that investment worthwhile. However, to do this comparison takes hundreds of hours each year in which analysts hand categorize each line-item. The goal here is to build a machine learning algorithm that can automate that process. For each line item, there are some text fields that tell us about the expense, for example, a line might say something like ‘Algebra books for 8th grade students’. There is also amount of the expenses in dollars. This line item then has a set of labels attached to it. For example, this one might have labels like ‘Textbooks’, ‘Math’, and Middle Schools. These labels are the target variables. This is a supervised learning problem where we want to use correctly labeled data to build an algorithm that can suggest labels for unlabeled lines. This is in contrast to an unsupervised learning problem where we don’t have labels and we are using an algorithm to automatically which line-items might go together. For this problem there are over 100 unique target variables that could be attached to a single line item. Because we want to predict a category for each line item, this is a classification problem. This is as opposed to a regression problem where we want to predict a numeric value for a line item, for example, predicting house prices. Some of the categories that we want to determine are the expenses like pre-kindergarten education – which is important because it has different funding sources.  Or is there a particular student_type that this expense supports. Overall there are 9 columns with many different possible categories in each column. It is impossible for humans to label these line items with 100% accuracy. We don’t want our algorithm to just say this line is for textbooks. We want it to say, its most likely this line is for textbooks and I am 60% sure that it is. If its not textbooks, I’m 30% sure its office supplies. By making these suggestions analysts can prioritize their time. This is called a human-in-the-loop machine learning system. We will predict a probability between 0 (the algorithm thinks this label is very unlikely for this line item) and 1 (the algorithm thinks this label is very likely). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What category of problem is this?\n",
    "\n",
    "let's make sure we agree on the basics.\n",
    "\n",
    "I am going to be working with school district budget data. This data can be classified in many ways according to certain labels, e.g. Function: Career & Academic Counseling, or Position_Type: Librarian.\n",
    "\n",
    "The goal is to develop a model that predicts the probability for each possible label by relying on some correctly labeled examples.\n",
    "\n",
    "What type of machine learning problem is this?\n",
    "\n",
    "Reinforcement Learning, because the model is learning from the data through a system of rewards and punishments.\n",
    "\n",
    "Unsupervised Learning, because the model doesn't output labels with certainty.\n",
    "\n",
    "Unsupervised Learning, because not all data is correctly classified to begin with.\n",
    "\n",
    "Supervised Learning, because the model will be trained using labeled examples.\n",
    "\n",
    "Using correctly labeled budget line items to train means this is a supervised learning problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is the goal of the algorithm?\n",
    "\n",
    "There are different types of supervised machine learning problems. Here I will identify what type of supervised machine learning problem this is, and why I think so.\n",
    "\n",
    "Remember, the goal is to correctly label budget line items by training a supervised model to predict the probability of each possible label, taking most probable label as the correct label.\n",
    "\n",
    "Regression, because the model will output probabilities.\n",
    "\n",
    "\n",
    "Classification, because predicted probabilities will be used to select a label class.\n",
    "\n",
    "Specifically, we have ourselves a multi-class-multi-label classification problem (quite a mouthful!), because there are 9 broad categories that each take on many possible sub-label instances.\n",
    "\n",
    "\n",
    "Regression, because probabilities take a continuous value between 0 and 1.\n",
    "\n",
    "\n",
    "Classification, because the model will output probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the Data \n",
    "\n",
    "Lastly I said that we’d be predicting probabilities for each budget line item. Let me clarify how it looks in practice. For example, if we are predicting the hair type and eye color of people. We have the categories “curly”, “straight” and “wavy” for hair and “brown” and “blue” for eyes. If we are predicting probabilities, we need a value for each possible value in each column. In this case the target would have the columns for each hair type and for each eye color. Later I will explore how to perform this transformation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading the data\n",
    "\n",
    "Now it's time to check out the dataset! I'll use pandas (which has been pre-imported as pd) to load the data into a DataFrame and then do some Exploratory Data Analysis (EDA) of it.\n",
    "\n",
    "The training data is available as TrainingData.csv. The first task is to load it into a DataFrame in the IPython Shell using pd.read_csv() along with the keyword argument index_col=0.\n",
    "\n",
    "Using methods such as .info(), .head(), and .tail() to explore the budget data and the properties of the features and labels.\n",
    "\n",
    "Some of the column names correspond to features - descriptions of the budget items - such as the Job_Title_Description column. The values in this column tell us if a budget item is for a teacher, custodian, or other employee.\n",
    "\n",
    "Some columns correspond to the budget item labels I will be trying to predict with the model. For example, the Object_Type column describes whether the budget item is related classroom supplies, salary, travel expenses, etc.\n",
    "\n",
    "Using df.info() in the IPython Shell to answer the following questions:\n",
    "\n",
    "How many rows are there in the training data?\n",
    "400277\n",
    "How many columns are there in the training data?\n",
    "25\n",
    "How many non-null entries are in the Job_Title_Description column?\n",
    "292743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "importing pandas as pd \n",
    "df = pd.read_csv('TrainingData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400277 entries, 134338 to 415831\n",
      "Data columns (total 25 columns):\n",
      "Function                  400277 non-null object\n",
      "Use                       400277 non-null object\n",
      "Sharing                   400277 non-null object\n",
      "Reporting                 400277 non-null object\n",
      "Student_Type              400277 non-null object\n",
      "Position_Type             400277 non-null object\n",
      "Object_Type               400277 non-null object\n",
      "Pre_K                     400277 non-null object\n",
      "Operating_Status          400277 non-null object\n",
      "Object_Description        375493 non-null object\n",
      "Text_2                    88217 non-null object\n",
      "SubFund_Description       306855 non-null object\n",
      "Job_Title_Description     292743 non-null object\n",
      "Text_3                    109152 non-null object\n",
      "Text_4                    53746 non-null object\n",
      "Sub_Object_Description    91603 non-null object\n",
      "Location_Description      162054 non-null object\n",
      "FTE                       126071 non-null float64\n",
      "Function_Description      342195 non-null object\n",
      "Facility_or_Department    53886 non-null object\n",
      "Position_Extra            264764 non-null object\n",
      "Total                     395722 non-null float64\n",
      "Program_Description       304660 non-null object\n",
      "Fund_Description          202877 non-null object\n",
      "Text_1                    292285 non-null object\n",
      "dtypes: float64(2), object(23)\n",
      "memory usage: 79.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>Use</th>\n",
       "      <th>Sharing</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Student_Type</th>\n",
       "      <th>Position_Type</th>\n",
       "      <th>Object_Type</th>\n",
       "      <th>Pre_K</th>\n",
       "      <th>Operating_Status</th>\n",
       "      <th>Object_Description</th>\n",
       "      <th>...</th>\n",
       "      <th>Sub_Object_Description</th>\n",
       "      <th>Location_Description</th>\n",
       "      <th>FTE</th>\n",
       "      <th>Function_Description</th>\n",
       "      <th>Facility_or_Department</th>\n",
       "      <th>Position_Extra</th>\n",
       "      <th>Total</th>\n",
       "      <th>Program_Description</th>\n",
       "      <th>Fund_Description</th>\n",
       "      <th>Text_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>134338</td>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KINDERGARTEN</td>\n",
       "      <td>50471.810</td>\n",
       "      <td>KINDERGARTEN</td>\n",
       "      <td>General Fund</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206341</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>CONTRACTOR SERVICES</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RGN  GOB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNDESIGNATED</td>\n",
       "      <td>3477.860</td>\n",
       "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326408</td>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Base Salary/Compensation</td>\n",
       "      <td>Non PreK</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>Personal Services - Teachers</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEACHER</td>\n",
       "      <td>62237.130</td>\n",
       "      <td>Instruction - Regular</td>\n",
       "      <td>General Purpose School</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364634</td>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Substitute</td>\n",
       "      <td>Benefits</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>EMPLOYEE BENEFITS</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNALLOC BUDGETS/SCHOOLS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>22.300</td>\n",
       "      <td>GENERAL MIDDLE/JUNIOR HIGH SCH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47683</td>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>TEACHER COVERAGE FOR TEACHER</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NON-PROJECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>54.166</td>\n",
       "      <td>GENERAL HIGH SCHOOL EDUCATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Function          Use          Sharing Reporting  \\\n",
       "134338     Teacher Compensation  Instruction  School Reported    School   \n",
       "206341                 NO_LABEL     NO_LABEL         NO_LABEL  NO_LABEL   \n",
       "326408     Teacher Compensation  Instruction  School Reported    School   \n",
       "364634  Substitute Compensation  Instruction  School Reported    School   \n",
       "47683   Substitute Compensation  Instruction  School Reported    School   \n",
       "\n",
       "       Student_Type Position_Type               Object_Type     Pre_K  \\\n",
       "134338     NO_LABEL       Teacher                  NO_LABEL  NO_LABEL   \n",
       "206341     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "326408  Unspecified       Teacher  Base Salary/Compensation  Non PreK   \n",
       "364634  Unspecified    Substitute                  Benefits  NO_LABEL   \n",
       "47683   Unspecified       Teacher   Substitute Compensation  NO_LABEL   \n",
       "\n",
       "         Operating_Status            Object_Description  ...  \\\n",
       "134338  PreK-12 Operating                           NaN  ...   \n",
       "206341      Non-Operating           CONTRACTOR SERVICES  ...   \n",
       "326408  PreK-12 Operating  Personal Services - Teachers  ...   \n",
       "364634  PreK-12 Operating             EMPLOYEE BENEFITS  ...   \n",
       "47683   PreK-12 Operating  TEACHER COVERAGE FOR TEACHER  ...   \n",
       "\n",
       "       Sub_Object_Description Location_Description  FTE  \\\n",
       "134338                    NaN                  NaN  1.0   \n",
       "206341                    NaN                  NaN  NaN   \n",
       "326408                    NaN                  NaN  1.0   \n",
       "364634                    NaN                  NaN  NaN   \n",
       "47683                     NaN                  NaN  NaN   \n",
       "\n",
       "           Function_Description Facility_or_Department  \\\n",
       "134338                      NaN                    NaN   \n",
       "206341                 RGN  GOB                    NaN   \n",
       "326408                      NaN                    NaN   \n",
       "364634  UNALLOC BUDGETS/SCHOOLS                    NaN   \n",
       "47683               NON-PROJECT                    NaN   \n",
       "\n",
       "                    Position_Extra      Total             Program_Description  \\\n",
       "134338               KINDERGARTEN   50471.810                    KINDERGARTEN   \n",
       "206341                UNDESIGNATED   3477.860   BUILDING IMPROVEMENT SERVICES   \n",
       "326408                     TEACHER  62237.130           Instruction - Regular   \n",
       "364634  PROFESSIONAL-INSTRUCTIONAL     22.300  GENERAL MIDDLE/JUNIOR HIGH SCH   \n",
       "47683   PROFESSIONAL-INSTRUCTIONAL     54.166   GENERAL HIGH SCHOOL EDUCATION   \n",
       "\n",
       "              Fund_Description                         Text_1  \n",
       "134338            General Fund                            NaN  \n",
       "206341                     NaN  BUILDING IMPROVEMENT SERVICES  \n",
       "326408  General Purpose School                            NaN  \n",
       "364634                     NaN            REGULAR INSTRUCTION  \n",
       "47683                      NaN            REGULAR INSTRUCTION  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are a lot of missing values. We will need to keep our eyes on those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summarizing the data\n",
    "\n",
    "I'll continue the EDA in this exercise by computing summary statistics for the numeric data in the dataset. \n",
    "\n",
    "We can use df.info() in the IPython Shell to determine which columns of the data are numeric, specifically type float64. You'll notice that there are two numeric columns, called FTE and Total.\n",
    "\n",
    "FTE: Stands for \"full-time equivalent\". If the budget item is associated to an employee, this number tells us the percentage of full-time that the employee works. A value of 1 means the associated employee works for the school full-time. A value close to 0 means the item is associated to a part-time or contracted employee.\n",
    "Total: Stands for the total cost of the expenditure. This number tells us how much the budget item cost.\n",
    "After printing summary statistics for the numeric data, your job is to plot a histogram of the non-null FTE column to see the distribution of part-time and full-time employees in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 FTE         Total\n",
      "count  126071.000000  3.957220e+05\n",
      "mean        0.426794  1.310586e+04\n",
      "std         0.573576  3.682254e+05\n",
      "min        -0.087551 -8.746631e+07\n",
      "25%         0.000792  7.379770e+01\n",
      "50%         0.130927  4.612300e+02\n",
      "75%         1.000000  3.652662e+03\n",
      "max        46.800000  1.297000e+08\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(df['FTE'].dropna())\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of %full-time \\n employee works')\n",
    "plt.xlabel('% of full-time')\n",
    "plt.ylabel('num employees')\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high variance in expenditures makes sense (some purchases are cheap some are expensive). Also, it looks like the FTE column is bimodal. That is, there are some part-time and some full-time employees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exploring datatypes in pandas\n",
    "\n",
    "It's always good to know what datatypes you're working with, especially when the inefficient pandas type object may be involved. Towards that end, let's explore what we have.\n",
    "\n",
    "The data has been loaded into the workspace as df. Your job is to look at the DataFrame attribute .dtypes in the IPython Shell, and call its .value_counts() method in order to answer the question below.\n",
    "\n",
    "Make sure to call df.dtypes.value_counts(), and not df.value_counts()! Check out the difference in the Shell. df.value_counts() will return an error, because it is a Series method, not a DataFrame method.\n",
    "\n",
    "How many columns with dtype object are in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     23\n",
       "float64     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of (slow) object types. Let's do some type conversion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Function',\n",
    " 'Use',\n",
    " 'Sharing',\n",
    " 'Reporting',\n",
    " 'Student_Type',\n",
    " 'Position_Type',\n",
    " 'Object_Type',\n",
    " 'Pre_K',\n",
    " 'Operating_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function            object\n",
       "Use                 object\n",
       "Sharing             object\n",
       "Reporting           object\n",
       "Student_Type        object\n",
       "Position_Type       object\n",
       "Object_Type         object\n",
       "Pre_K               object\n",
       "Operating_Status    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[LABELS].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            category\n",
      "Use                 category\n",
      "Sharing             category\n",
      "Reporting           category\n",
      "Student_Type        category\n",
      "Position_Type       category\n",
      "Object_Type         category\n",
      "Pre_K               category\n",
      "Operating_Status    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[LABELS] = df[LABELS].apply(categorize_label, axis=0)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[LABELS].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WE're getting close to something we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Counting unique labels\n",
    "\n",
    "There are over 100 unique labels. Here I will explore this fact by counting and plotting the number of unique values for each category of label.\n",
    "\n",
    "The dataframe df and the LABELS list have been loaded into the workspace; the LABELS columns of df have been converted to category types.\n",
    "\n",
    "pandas, which has been pre-imported as pd, provides a pd.Series.nunique method for counting the number of unique values in a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate number of unique values for each label: num_unique_labels\n",
    "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
    "\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind='bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How do we measure success?\n",
    "\n",
    "Choosing how to evaluate the machine learning model is one of the most important decisions an analyst makes. The decision balances the real-world use of the algorithm, the mathematical properties of the evaluation function, and the interpretability of the measure. Often we hear how accurate is the model. Accuracy is a simple measure that tells us what percentage of rows we got right. However, sometimes accuracy doesn’t tell the whole story. \n",
    "\n",
    "In some cases we use the metric log loss. Log loss is what is generally called a “loss function” and it is a measure of error. The error should be as small as possible, which is the opposite of a metric like accuracy where we want to maximize the value. Log loss takes the actual value 1 or 0 and takes our prediction which is a probability between 0 and 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Penalizing highly confident wrong answers\n",
    "\n",
    "log loss provides a steep penalty for predictions that are both wrong and confident, i.e., a high probability is assigned to the incorrect class.\n",
    "\n",
    "Suppose you have the following 3 examples:\n",
    "\n",
    "A:y=1,p=0.85\n",
    "    \n",
    "B:y=0,p=0.99\n",
    "    \n",
    "C:y=0,p=0.51\n",
    "    \n",
    "Select the ordering of the examples which corresponds to the lowest to highest log loss scores. y is an indicator of whether the example was classified correctly. You shouldn't need to crunch any numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a - Lowest: A, Middle: B, Highest: C.\n",
    "            \n",
    "\n",
    "b - Lowest: C, Middle: A, Highest: B.\n",
    "            \n",
    "\n",
    "c- Lowest: A, Middle: C, Highest: B.\n",
    "This is correct. \n",
    "            \n",
    "\n",
    "d - Lowest: B, Middle: A, Highest: C.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confident and wrong predictions are highly penalized, resulting in a higher log loss.\n",
    "\n",
    "Of the two incorrect predictions, B will have a higher log loss because it is confident and wrong.\n",
    "\n",
    "B is both wrong and confident. It will not have the lowest log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing log loss with NumPy\n",
    "\n",
    "To see how the log loss metric handles the trade-off between accuracy and confidence, we will use some sample data generated with NumPy and compute the log loss using the provided function compute_log_loss(). \n",
    "\n",
    "5 one-dimensional numeric arrays simulating different types of predictions have been pre-loaded: actual_labels, correct_confident, correct_not_confident, wrong_not_confident, and wrong_confident.\n",
    "\n",
    "The job is to compute the log loss for each sample set provided using the compute_log_loss(predicted_values, actual_values). It takes the predicted values as the first argument and the actual values as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "correct_confident = np.array([0.95, 0.95, 0.95, 0.95, 0.95, 0.05, 0.05, 0.05, 0.05, 0.05])\n",
    "\n",
    "correct_not_confident = np.array([0.65, 0.65, 0.65, 0.65, 0.65, 0.35, 0.35, 0.35, 0.35, 0.35])\n",
    "\n",
    "wrong_not_confident = np.array([0.35, 0.35, 0.35, 0.35, 0.35, 0.65, 0.65, 0.65, 0.65, 0.65])\n",
    "\n",
    "wrong_confident = np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.95, 0.95, 0.95, 0.95, 0.95])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing log loss with NumPy\n",
    "\n",
    "import numpy as np\n",
    "def compute_log_loss(predicted, actual, eps=0.00000000000001):\n",
    "    \"\"\" Computes the logarithmic loss between predicted and \n",
    "        actual when these are 1D arrays. \n",
    "\n",
    "      : param predicted: The predicted probabilities as floats between 0-1\n",
    "      : param actual: The actual binary labels, Either 0 or 1.\n",
    "      : param eps (optional): log(0) is inf, so we need to offset our\n",
    "                              predicted values slightly by eps from 0 or 1. \n",
    "    \"\"\"\n",
    "    predicted = np.clip(predicted, eps, 1 - eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted)\n",
    "              + (1 - actual)\n",
    "              * np.log(1 - predicted))\n",
    "    return loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss, correct and confident: 0.05129329438755058\n",
      "Log loss, correct and not confident: 0.4307829160924542\n",
      "Log loss, wrong and not confident: 1.049822124498678\n",
      "Log loss, wrong and confident: 2.9957322735539904\n",
      "Log loss, actual labels: 9.99200722162646e-15\n"
     ]
    }
   ],
   "source": [
    "# Compute and print log loss for 1st case\n",
    "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
    "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss)) \n",
    "\n",
    "# Compute log loss for 2nd case\n",
    "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
    "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 3rd case\n",
    "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
    "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 4th case\n",
    "wrong_confident_loss = compute_log_loss(wrong_confident, actual_labels)\n",
    "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for actual labels\n",
    "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
    "print(\"Log loss, actual labels: {}\".format(actual_labels_loss)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log loss penalizes highly confident wrong answers much more than any other type. This will be a good metric to use on the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating a simple first model\n",
    "\n",
    "I'll build a first-pass model. I'll use numeric data only to train the model. Spoiler alert - throwing out all of the text data is bad for performance! But I'll learn how to format the predictions. Then, I'llintroduce natural language processing (NLP) in order to start working with the large amounts of text in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### It’s time to build a model\n",
    "\n",
    "It’s always a good approach to start from a very simple model. I will start with a model that will just use the numeric data columns. I’ll go from raw data to predictions as quickly as possible. I’ll use multi-class logistic regression which treats each label as independent. The model will train a logistic regression classifier for each of these columns separately and then use those models to predict whether the label appears or not for any given rows. First the data is split into a training set and a test set. However, because of the nature of the data here, the simple approach to a train-test split won’t work. Some labels only appear in a small fraction of the dataset. If I split the dataset randomly I may end up with labels in the dataset that never appeared in the training set. The model will never be able to predict a class that it has never seen before. One approach to this problem is called StratifiedShufleSplit. However this scikit-learn function only works if we have a single target variable. In this case we have many target variables. To work around this issue, there is a utility function: multilabel_train_test_split() that will ensure that all of the classes are represented in both the test and training sets. \n",
    "\n",
    "First I’ll subset the data to just the numeric columns. NUMERIC_COLUMNS is a variable that will contain a list of the column names for the columns that are numbers rather than text. Then I will do a minimal amount of preprocessing where I’ll fill the NaNs that are in the dataset with -1000. I chose -1000 to let the algorithm respond to NaN’s differently than 0. I’ll create the array of target variables using the get dummies function is pandas. Again the get_dummies function takes the categories and produces a binary indicator for our targets which is the format that scikit-learn needs to build a model. Finally I will use the multilabel_train_test_split function that is designed to split the dataset in to a training set and a test set. \n",
    "\n",
    "I’ll import LogisticRegression and OneVsRestClassifier from the sklearn.multiclass module. OneVsRest lets us treat each column of y independently. Essentially it fits a separate classifier for each of the columns. This is just one strategy we can use if we have multiple classes. We can find other strategies at the scikit-learn documentation for other strategies. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Setting up a train-test split in scikit-learn\n",
    "\n",
    "It's finally time to start training models!\n",
    "\n",
    "The first step is to split the data into a training set and a test set. Some labels don't occur very often, but we want to make sure that they appear in both the training and the test sets. We provide a function that will make sure at least min_count examples of each label appear in each split: multilabel_train_test_split.\n",
    "\n",
    "I'll start with a simple model that uses just the numeric columns of the DataFrame when calling multilabel_train_test_split. The data has been read into a DataFrame df and a list consisting of just the numeric columns is available as NUMERIC_COLUMNS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = ['FTE', 'Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
    "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
    "        the indices for a sample of size `size` if\n",
    "        `size` > 1 or `size` * len(y) if size =< 1.\n",
    "        The sample is guaranteed to have > `min_count` of\n",
    "        each label.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n",
    "            raise ValueError()\n",
    "    except (TypeError, ValueError):\n",
    "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
    "\n",
    "    if (y.sum(axis=0) < min_count).any():\n",
    "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
    "\n",
    "    if size <= 1:\n",
    "        size = np.floor(y.shape[0] * size)\n",
    "\n",
    "    if y.shape[1] * min_count > size:\n",
    "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
    "        warn(msg.format(y.shape[1] * min_count, size))\n",
    "        size = y.shape[1] * min_count\n",
    "\n",
    "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        choices = y.index\n",
    "        y = y.values\n",
    "    else:\n",
    "        choices = np.arange(y.shape[0])\n",
    "\n",
    "    sample_idxs = np.array([], dtype=choices.dtype)\n",
    "\n",
    "    # first, guarantee > min_count of each label\n",
    "    for j in range(y.shape[1]):\n",
    "        label_choices = choices[y[:, j] == 1]\n",
    "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
    "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
    "\n",
    "    sample_idxs = np.unique(sample_idxs)\n",
    "\n",
    "    # now that we have at least min_count of each, we can just random sample\n",
    "    sample_count = int(size - sample_idxs.shape[0])\n",
    "\n",
    "    # get sample_count indices from remaining choices\n",
    "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
    "    remaining_sampled = rng.choice(remaining_choices,\n",
    "                                   size=sample_count,\n",
    "                                   replace=False)\n",
    "\n",
    "    return np.concatenate([sample_idxs, remaining_sampled])\n",
    "\n",
    "\n",
    "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
    "        classes in the binary matrix `labels` are represented at\n",
    "        least `min_count` times.\n",
    "    \"\"\"\n",
    "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
    "    return df.loc[idxs]\n",
    "\n",
    "\n",
    "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
    "        returns (X_train, X_test, Y_train, Y_test) where all\n",
    "        classes in Y are represented at least `min_count` times.\n",
    "    \"\"\"\n",
    "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
    "\n",
    "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
    "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
    "\n",
    "    test_set_mask = index.isin(test_set_idxs)\n",
    "    train_set_mask = ~test_set_mask\n",
    "\n",
    "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 320222 entries, 134338 to 415831\n",
      "Data columns (total 2 columns):\n",
      "FTE      320222 non-null float64\n",
      "Total    320222 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 7.3 MB\n",
      "None\n",
      "\n",
      "X_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80055 entries, 206341 to 72072\n",
      "Data columns (total 2 columns):\n",
      "FTE      80055 non-null float64\n",
      "Total    80055 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 1.8 MB\n",
      "None\n",
      "\n",
      "y_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 320222 entries, 134338 to 415831\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 34.2 MB\n",
      "None\n",
      "\n",
      "y_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80055 entries, 206341 to 72072\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 8.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the new DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,label_dummies,size=0.2,seed=123)\n",
    "\n",
    "# Print the info\n",
    "print(\"X_train info:\")\n",
    "print(X_train.info())\n",
    "print(\"\\nX_test info:\")  \n",
    "print(X_test.info())\n",
    "print(\"\\ny_train info:\")  \n",
    "print(y_train.info())\n",
    "print(\"\\ny_test info:\")  \n",
    "print(y_test.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training a model\n",
    "\n",
    "With split data in hand, we're only a few lines away from training a model.\n",
    "\n",
    "I will import the logistic regression and one versus rest classifiers in order to fit a multi-class logistic regression model to the NUMERIC_COLUMNS of the feature data.\n",
    "\n",
    "Then I'll test and print the accuracy with the .score() method to see the results of training.\n",
    "\n",
    "Before we train! Remember, we're ultimately going to be using logloss to score our model, so we don't need to worry too much about the accuracy here. Keep in mind that we're throwing away all of the text data in the dataset - that's by far most of the data! So don't get your hopes up for a killer performance just yet. We're just interested in getting things up and running at the moment.\n",
    "\n",
    "All data necessary to call multilabel_train_test_split() has been loaded into the workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Create the DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
    "                                                               label_dummies,\n",
    "                                                               size=0.2, \n",
    "                                                               seed=123)\n",
    "\n",
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! The good news is that the workflow didn't cause any errors. The bad news is that the model scored the lowest possible accuracy: 0.0! But hey, we just threw away ALL of the text data in the budget. Later, we won't. Before I add the text data, let's see how the model does when scored by log loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using the model to predict values on holdout data\n",
    "\n",
    "Time to make some predictions! Remember, the train-test-split you've carried out so far is for model development. The original competition provides an additional test set, for which I'll never actually see the correct labels. This is called the \"holdout data.\"\n",
    "\n",
    "The point of the holdout data is to provide a fair test for machine learning competitions. If the labels aren't known by anyone but DataCamp, DrivenData, or whoever is hosting the competition, we can be sure that no one submits a mere copy of labels to artificially pump up the performance on their model.\n",
    "\n",
    "Remember that the original goal is to predict the probability of each label. In this exercise I'll do just that by using the .predict_proba() method on the trained model.\n",
    "\n",
    "First, however, I'll need to load the holdout data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout = pd.read_csv('TestData.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50064 entries, 180042 to 249087\n",
      "Data columns (total 16 columns):\n",
      "Object_Description        48330 non-null object\n",
      "Program_Description       44811 non-null object\n",
      "SubFund_Description       16111 non-null object\n",
      "Job_Title_Description     32317 non-null object\n",
      "Facility_or_Department    2839 non-null object\n",
      "Sub_Object_Description    33612 non-null object\n",
      "Location_Description      37316 non-null object\n",
      "FTE                       19605 non-null float64\n",
      "Function_Description      46866 non-null object\n",
      "Position_Extra            13813 non-null object\n",
      "Text_4                    2814 non-null object\n",
      "Total                     49404 non-null float64\n",
      "Text_2                    4641 non-null object\n",
      "Text_3                    5784 non-null object\n",
      "Fund_Description          39586 non-null object\n",
      "Text_1                    15378 non-null object\n",
      "dtypes: float64(2), object(14)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "holdout.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = ['FTE', 'Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit it to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Load the holdout data: holdout\n",
    "# The data is loaded above.\n",
    "\n",
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Writing out the results to a csv for submission\n",
    "\n",
    "At last, we're ready to submit some predictions for scoring. I'll write the predictions to a .csv using the .to_csv() method on a pandas DataFrame. Then I'll evaluate the performance according to the LogLoss metric discussed earlier!\n",
    "\n",
    "I'll need to make sure your submission obeys the correct format.\n",
    "\n",
    "To do this, I'll use the predictions values to create a new DataFrame, prediction_df.\n",
    "\n",
    "Interpreting LogLoss & Beating the Benchmark:\n",
    "\n",
    "When interpreting the log loss score, we have to keep in mind that the score will change based on the number of samples tested. To get a sense of how this very basic model performs, we can compare the score to the DrivenData benchmark model performance: 2.0455, which merely submitted uniform probabilities for each class.\n",
    "\n",
    "Remember, the lower the log loss the better. Is our model's log loss lower than 2.0455?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('HoldoutData.csv', <http.client.HTTPMessage at 0x1137dca50>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_HOLDOUT_DATA = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetSample.csv'\n",
    "PATH_TO_HOLDOUT_LABELS = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetLabelsSample.csv'\n",
    "fn = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetSample.csv'\n",
    "from urllib.request import urlretrieve\n",
    "urlretrieve(fn, 'HoldoutData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = ['FTE', 'Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression(solver='liblinear'))\n",
    "\n",
    "# Fit it to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Load the holdout data: holdout\n",
    "holdout = pd.read_csv('HoldoutData.csv',index_col=0)\n",
    "\n",
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load path to pred\n",
    "PATH_TO_PREDICTIONS = \"predictions.csv\"\n",
    "# Load path to holdout\n",
    "PATH_TO_HOLDOUT_DATA = \"https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetSample.csv\"\n",
    "PATH_TO_HOLDOUT_LABELS = \"https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetLabelsSample.csv\"\n",
    "\n",
    "# SCORING UTILITIES\n",
    "\n",
    "BOX_PLOTS_COLUMN_INDICES = [range(37),\n",
    " range(37,48),\n",
    " range(48,51),\n",
    " range(51,76),\n",
    " range(76,79),\n",
    " range(79,82),\n",
    " range(82,87),\n",
    " range(87,96),\n",
    " range(96,104)]\n",
    "\n",
    "def _multi_multi_log_loss(predicted,\n",
    "    actual,\n",
    "    class_column_indices=BOX_PLOTS_COLUMN_INDICES,\n",
    "    eps=1e-15):\n",
    "\n",
    "    class_scores = np.ones(len(class_column_indices), dtype=np.float64)\n",
    "\n",
    "    # calculate log loss for each set of columns that belong to a class:\n",
    "    for k, this_class_indices in enumerate(class_column_indices):\n",
    "        # get just the columns for this class\n",
    "        preds_k = predicted[:, this_class_indices].astype(np.float64)\n",
    "\n",
    "        # normalize so probabilities sum to one (unless sum is zero, then we clip)\n",
    "        preds_k /= np.clip(preds_k.sum(axis=1).reshape(-1, 1), eps, np.inf)\n",
    "\n",
    "        actual_k = actual[:, this_class_indices]\n",
    "\n",
    "        # shrink predictions so\n",
    "        y_hats = np.clip(preds_k, eps, 1 - eps)\n",
    "        sum_logs = np.sum(actual_k * np.log(y_hats))\n",
    "        class_scores[k] = (-1.0 / actual.shape[0]) * sum_logs\n",
    "\n",
    "        return np.average(class_scores)\n",
    "\n",
    "\n",
    "def score_submission(pred_path=PATH_TO_PREDICTIONS, holdout_path=PATH_TO_HOLDOUT_LABELS):\n",
    "    # this happens on the backend to get the score\n",
    "    holdout_labels = pd.get_dummies(\n",
    "    pd.read_csv(holdout_path, index_col=0)\n",
    "    .apply(lambda x: x.astype('category'), axis=0)\n",
    "    )\n",
    "\n",
    "    preds = pd.read_csv(pred_path, index_col=0)\n",
    "\n",
    "    # make sure that format is correct\n",
    "    assert (preds.columns == holdout_labels.columns).all()\n",
    "    assert (preds.index == holdout_labels.index).all()\n",
    "\n",
    "    return _multi_multi_log_loss(preds.values, holdout_labels.values)\n",
    "\n",
    "# Load holdout data\n",
    "holdout = pd.read_csv(PATH_TO_HOLDOUT_DATA, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model, trained with numeric data only, yields logloss score: 1.2284639067368301\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
    "\n",
    "# Format predictions in DataFrame: prediction_df\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
    "                             index=holdout.index,\n",
    "                             data=predictions)\n",
    "\n",
    "\n",
    "# Save prediction_df to csv\n",
    "prediction_df.to_csv('predictions.csv')\n",
    "\n",
    "# Submit the predictions for scoring: score\n",
    "score = score_submission(pred_path='predictions.csv')\n",
    "\n",
    "# Print score\n",
    "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our basic model scored 0.0 accuracy. We've now got the basics down and have made a first pass at this complicated supervised learning problem. It's time to step up the game and incorporate the text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A very brief introduction to NLP\n",
    "\n",
    "When we have data that is text we often want to process this text to create features for our algorithms. This is called Natural Language Processing or NLP. The first step in processing text data is called “tokenization”. Tokenization is the process of splitting a long string into segments. This means taking a string and splitting it into a list of strings where we have one string for each word. For example, we might split the string “Natural language Processing”  into a list of three separate tokens: “Natural”, “Language”, and “Processing”. \n",
    "\n",
    "In the school data we have the full string “PETRO-VEND FUEL AND FLUIDS”. If we want to tokenize on whitespace, that is split into words every time there is a space, tab, or return in the text, we end up with 4 tokens. The token “PETRO-VEND”, the token “FUEL”, the token “AND” and the token “FLUIDS”. For some datasets, we may want to split words based on other characters than whitespace. There may be words in the dataset that are combined with a hyphen, like PETRO-VEND, we can opt in this case to tokenize on whitespace and punctuation. Here we break into tokens every time we see a space or any mark of punctuation, in this case we end up with 5 tokens, “PETRO” and “VEND” become two separate tokens. After tokenizing we want to use them as part of our machine learning algorithm. Often the first way to do this is to simply count the number of times that a particular token appears in a row. This is called a “bag of words” representation because we can imagine our vocabulary as a bag of words and we just count the number of times a particular word was pulled out of that bag. This approach discards the information about the word order. That is the phrase “red not blue” would be treated the same “blue not red”. \n",
    "\n",
    "A slightly more sophisticated approach is to create what are called “n-grams”. In addition to a column for every token we see which is called “1-gram”, we may have a column for every ordered pair of two words. In that case we will have a column for “PETRO-VEND”, a column for “VEND FUEL”, a column for “FUEL AND”, a column for “AND FLUIDS”. These are called 2-grams or bi-grams. We can also have 3-grams or tri-grams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tokenizing text\n",
    "\n",
    "Tokenization is the process of chopping up a character sequence into pieces called tokens.\n",
    "\n",
    "How do we determine what constitutes a token? Often, tokens are separated by whitespace. But we can specify other delimiters as well. For example, if we decided to tokenize on punctuation, then any punctuation mark would be treated like a whitespace. How we tokenize text in our DataFrame can affect the statistics we use in our model.\n",
    "\n",
    "A particular cell in our budget DataFrame may have the string content Title I - Disadvantaged Children/Targeted Assistance. The number of n-grams generated by this text data is sensitive to whether or not we tokenize on punctuation.\n",
    "\n",
    "How many tokens (1-grams) are in the string\n",
    "\n",
    "Title I - Disadvantaged Children/Targeted Assistance\n",
    "\n",
    "if we tokenize on whitespace and punctuation?\n",
    "\n",
    "6\n",
    "\n",
    "Tokenizing on whitespace and punctuation means that Children/Targeted becomes two tokens and - is dropped altogether. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing the NLP credentials with n-grams\n",
    "\n",
    "We're well on your way to NLP superiority. Let's test our mastery of n-grams!\n",
    "\n",
    "In the workspace, we have the loaded a python list, one_grams, which contains all 1-grams of the string petro-vend fuel and fluids, tokenized on punctuation. Specifically,\n",
    "\n",
    "one_grams = ['petro', 'vend', 'fuel', 'and', 'fluids']\n",
    "\n",
    "Here I will try to determine the sum of the sizes of 1-grams, 2-grams and 3-grams generated by the string petro-vend fuel and fluids, tokenized on punctuation.\n",
    "\n",
    "Recall that the n-gram of a sequence consists of all ordered subsequences of length n.\n",
    "\n",
    "The number of 1-grams + 2-grams + 3-grams is 5 + 4 + 3 = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Representing text numerically\n",
    "\n",
    "Here I will turn the tokenization into an array that we can feed into a machine learning algorithm. Bag of words is one of the simplest ways to represent text in a machine learning algorithm. It discards information about grammar and word order just assuming that the number of times a word occurs is enough information. Scikit-learn provides a very useful tool for creating bag-of-words representations. It is called the CountVectorizer. The CountVectorizer works by taking an array of strings and doing three things. First it tokenizes all of the strings. Then it makes note of all of the words that appear which we call the “vocabulary”. Finally it counts the number of times that each token in the vocabulary appears in every given row. \n",
    "\n",
    "After importing the CountVectorizer I will define a regular expression that does a split on whitespace. I’ll also make sure that the text column does not have any NaN values, simply replacing those with empty strings instead. Finally, I will create a CountVectorizer object where I will pass in the token_pattern that I have created. This creates an object that we can use to create bag-of-words representations of text. The CountVectorizer object can be used with the fit and transform pattern just like any other preprocessor in scikit-learn. Fit will parse all of the strings for tokens and then create the vocabulary. Vocabulary specifically means all of the tokens that appear in this dataset. Transform will tokenize the text and then produce the array of counts. I will look at how changing the token pattern will change the number of tokens that the CountVectorizer recognizes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating a bag-of-words in scikit-learn\n",
    "\n",
    "I'll study the effects of tokenizing in different ways by comparing the bag-of-words representations resulting from different token patterns.\n",
    "\n",
    "I will focus on one feature only, the Position_Extra column, which describes any additional information not captured by the Position_Type label.\n",
    "\n",
    "For example, in the Shell you can check out the budget item in row 8960 of the data using df.loc[8960]. Looking at the output reveals that this Object_Description is overtime pay. For who? The Position Type is merely \"other\", but the Position Extra elaborates: \"BUS DRIVER\". Explore the column further to see more instances. It has a lot of NaN values.\n",
    "\n",
    "The task here is to turn the raw text in this column into a bag-of-words representation by creating tokens that contain only alphanumeric characters.\n",
    "\n",
    "For comparison purposes, the first 15 tokens of vec_basic, which splits df.Position_Extra into tokens when it encounters only whitespace characters, have been printed along with the length of the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Fill missing values in df.Position_Extra\n",
    "df.Position_Extra.fillna('', inplace=True)\n",
    "\n",
    "# Instantiate the CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_alphanumeric.fit(df.Position_Extra)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
    "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
    "print(vec_alphanumeric.get_feature_names()[:15])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating only alpha-numeric characters as tokens gives us a smaller number of more meaningful tokens. We've got bag-of-words in the bag!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combining text columns for tokenization\n",
    "\n",
    "In order to get a bag-of-words representation for all of the text data in our DataFrame, we must first convert the text data in each row of the DataFrame into a single string.\n",
    "\n",
    "Above this wasn't necessary because you only looked at one column of data, so each row was already just a single string. CountVectorizer expects each row to just be a single string, so in order to use all of the text columns, we'll need a method to turn a list of strings into a single string.\n",
    "\n",
    "I'll complete the function definition combine_text_columns(). When completed, this function will convert all training text data in our DataFrame to a single string per row that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method.\n",
    "\n",
    "Note that the function uses NUMERIC_COLUMNS and LABELS to determine which columns to drop. These lists have been loaded into the workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define combine_text_columns()\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis=1)\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna(\"\", inplace=True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll put this function to use in the next exercise to tokenize the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What's in a token?\n",
    "\n",
    "Now we will use combine_text_columns to convert all training text data in the DataFrame to a single vector that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method.\n",
    "\n",
    "I'll compare the effect of tokenizing using any non-whitespace characters as a token and using only alphanumeric characters as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4757 tokens in the dataset\n",
      "There are 3284 alpha-numeric tokens in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the basic token pattern\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# Create the alphanumeric token pattern\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate basic CountVectorizer: vec_basic\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
    "\n",
    "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(df)\n",
    "\n",
    "# Fit and transform vec_basic\n",
    "vec_basic.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_basic\n",
    "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
    "\n",
    "# Fit and transform vec_alphanumeric\n",
    "vec_alphanumeric.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_alphanumeric\n",
    "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pipelines, feature & text preprocessing\n",
    "\n",
    "Now its time to combine NLP with the model pipeline and incorporate the text data into the algorithm. Pipeline is a repeatable way to go from raw data to a trained machine learning model. The scikit-learn Pipeline object takes a sequential list of steps where the output of one step is the input of the next. Each step is represented with a name for the step that is simply a string and an object that implements the fit and transform methods. CountVectroizer is a good example that I used above. Pipelines are a very flexible way to represent the workflow. In fact, we can even have a sub-pipeline as one of the steps. The beauty of the Pipeline is that it encapsulates every transformation from raw data to a trained model. We create a pipeline by passing it a series of named steps. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Instantiate pipeline\n",
    "\n",
    "In order to make our life easier as we start to work with all of the data in your original DataFrame, df, it's time to turn to one of scikit-learn's most useful objects: the Pipeline.\n",
    "\n",
    "In some subsequent codes, I will work pipelines and train a classifier on some synthetic (sample) data of multiple datatypes before using the same techniques on the main dataset.\n",
    "\n",
    "The sample data is stored in the DataFrame, sample_df, which has three kinds of feature data: numeric, text, and numeric with missing values. It also has a label column with two classes, a and b.\n",
    "\n",
    "The job is to instantiate a pipeline that trains using the numeric column of the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric</th>\n",
       "      <th>text</th>\n",
       "      <th>with_missing</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-10.856306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.433240</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.973454</td>\n",
       "      <td>foo</td>\n",
       "      <td>4.310229</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.829785</td>\n",
       "      <td>foo bar</td>\n",
       "      <td>2.468976</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.258769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.437678</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.874678</td>\n",
       "      <td>foo bar</td>\n",
       "      <td>2.678909</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     numeric     text  with_missing label\n",
       "0 -10.856306      NaN      4.433240     b\n",
       "1   9.973454      foo      4.310229     a\n",
       "2   2.829785  foo bar      2.468976     b\n",
       "3   2.258769      NaN      2.437678     b\n",
       "4   1.874678  foo bar      2.678909     a"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.read_csv('sample.csv', na_values='')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - numeric, no nans:  0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import other necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Split and select numeric data only, no nans \n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=22)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now it's time to incorporate numeric data with missing values by adding a preprocessing step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing numeric features\n",
    "\n",
    "What would have happened if we had included the with 'with_missing' column in the last exercise? Without imputing missing values, the pipeline would not be happy (try it and see). So, I'll improve the pipeline a bit by using the Imputer() imputation transformer from scikit-learn to fill in missing values in the sample data.\n",
    "\n",
    "By default, the imputer transformer replaces NaNs with the mean value of the column. That's a good enough imputation strategy for the sample data, so we won't need to pass anything extra to the imputer.\n",
    "\n",
    "After importing the transformer, we will edit the steps list used in above codes by inserting a (name, transform) tuple. Recall that steps are processed sequentially, so we have to make sure the new tuple encoding our preprocessing step is put in the right place.\n",
    "\n",
    "The sample_df is in the workspace, in case you'd like to take another look. Make sure to select both numeric columns- in the previous codes we couldn't use with_missing because we had no preprocessing step!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - all numeric, incl nans:  0.548\n"
     ]
    }
   ],
   "source": [
    "# Import the Imputer object\n",
    "from sklearn.preprocessing import Imputer \n",
    "\n",
    "# Create training and test sets using only numeric data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric','with_missing']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=456)\n",
    "\n",
    "# Insantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! we know how to use preprocessing in pipelines with numeric data, and it looks like the accuracy has improved because of it! Text data preprocessing is next!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text features and feature union\n",
    "\n",
    "Now I will pre-process text data in a pipeline and then put it all together. The sample_data contains a text column as well that I haven’t used yet. Now the pipeline will use the CountVectorizer on the dataset and then it will pass the result into the classifier. We can’t have a pipeline that has a CountVectorizer step, Imputation step and then a classifier. The CountVectorizer doesn’t work with the numeric columns and we don’t want to use the imputer on text columns. In order to build the pipeline we need to separately operate on the text columns and the numeric columns. There are two tools: FunctionTransformer and FeatureUnion that helps to build the pipeline to work with both the text and numeric data. First I will use FunctionTransformer. It has a simple job: to take a python function and turn it into an object that scikit-learn pipeline can understand. \n",
    "\n",
    "I will write two simple functions: one that takes the whole dataframe and returns just the numeric columns. The other takes the whole dataframe and returns just the text columns. Using these function transformers we can build a separate pipeline for numeric data and text data. \n",
    "\n",
    "First we do train test split on the entire dataset and import FunctionTransformer and FeatureUnion utilities. Next, I will create two FunctionTransformer objects. The first takes a dataframe and returns the column named text. The second takes a datafrmae and returns the columns named numeric and with_missing. These two function transformer objects will let us set up separate pipelines that operate on the selected columns only. By passing the parameter validate equals False it tells to scikit-learn to not check for NaNs or validate the dtypes of the input. I will do that myself. FeatureUnion from the sklearn.pipeline module is the other utility we need. We know that text pipeline generates the array with text features. Numeric pipeline generates the array with numeric features. The FeatureUnion object puts these two sets of features together as a single array that will be the input to the classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing text features\n",
    "\n",
    "Here, I'll perform a similar preprocessing pipeline step, only this time I'll use the text column from the sample data.\n",
    "\n",
    "To preprocess the text, I'll turn to CountVectorizer() to generate a bag-of-words representation of the data. Using the default arguments, adding a (step, transform) tuple to the steps list in the pipeline.\n",
    "\n",
    "Making sure we select only the text column for splitting our training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - just text data:  0.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Split out only the text data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'].fillna(''),\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=456)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('vec', CountVectorizer()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - just text data: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm ready to create a pipeline for processing multiple datatypes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multiple types of processing: FunctionTransformer\n",
    "    \n",
    "Next I will introduce new topics we'll need to make our pipeline truly excel.\n",
    "\n",
    "Any step in the pipeline must be an object that implements the fit and transform methods. The FunctionTransformer creates an object with these methods out of any Python function that we pass to it. We'll use it to help select subsets of data in a way that plays nicely with pipelines.\n",
    "\n",
    "We are working with numeric data that needs imputation, and text data that needs to be converted into a bag-of-words. I'll create functions that separate the text from the numeric variables and see how the .fit() and .transform() methods work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data\n",
      "0        NaN\n",
      "1        foo\n",
      "2    foo bar\n",
      "3        NaN\n",
      "4    foo bar\n",
      "Name: text, dtype: object\n",
      "\n",
      "Numeric Data\n",
      "     numeric  with_missing\n",
      "0 -10.856306      4.433240\n",
      "1   9.973454      4.310229\n",
      "2   2.829785      2.468976\n",
      "3   2.258769      2.437678\n",
      "4   1.874678      2.678909\n"
     ]
    }
   ],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Obtain the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
    "\n",
    "# Obtain the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
    "\n",
    "# Fit and transform the text data: just_text_data\n",
    "just_text_data = get_text_data.fit_transform(sample_df)\n",
    "\n",
    "# Fit and transform the numeric data: just_numeric_data\n",
    "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
    "\n",
    "# Print head to check results\n",
    "print('Text Data')\n",
    "print(just_text_data.head())\n",
    "print('\\nNumeric Data')\n",
    "print(just_numeric_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multiple types of processing: FeatureUnion\n",
    "    \n",
    "Now that we can separate text and numeric data in the pipeline, we're ready to perform separate steps on each by nesting pipelines and using FeatureUnion().\n",
    "\n",
    "These tools will allow us to streamline all preprocessing steps for our model, even when multiple datatypes are involved. Here, for example, we don't want to impute our text data, and we don't want to create a bag-of-words with our numeric data. Instead, we want to deal with these separately and then join the results together using FeatureUnion().\n",
    "\n",
    "In the end, I'll still have only two high-level steps in our pipeline: preprocessing and model instantiation. The difference is that the first preprocessing step actually consists of a pipeline for numeric data and a pipeline for text data. The results of those pipelines are joined using FeatureUnion().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - all data:  0.568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "sample_df.text = sample_df['text'].fillna('')\n",
    "\n",
    "# Split using ALL data in sample_df\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=22)\n",
    "\n",
    "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
    "process_and_join_features = FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "# Instantiate nested pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', process_and_join_features),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "\n",
    "# Fit pl to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all data: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know more about pipelines than many practicing data scientists. You're on fire!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Choosing a classification model\n",
    "\n",
    "The main dataset has 14 text columns. Above I wrote a function combine_text_columns that combines all text columns into a single column. We will re-use this function as part of our Pipeline to process the text in the budget dataset. I will create a FunctionTransformer object using the combine_text_columns function instead of the simple selection function I used on the sample dataset. Other than this one change, the pipeline that processes the text imputes the numeric columns, joins these results together and then fits a multilabel logistic regression remains the same. After that I will have an infrastructure for processing the features and fitting a model. This infrastructure allows us to easily experiment with adapting different parts of the pipeline. \n",
    "\n",
    "I will try to improve the performance of the pipeline. I will try to experiment with different classes of models. All of the pipeline code stays the same but we can update the last step to be a different class of model instead of LogisticRegression. For example, we could try a RandomForestClassifier, NaiveBayesClassifier, or a KNeighborsClassifier instead. We can look at the scikit-learn documentation and choose any model class we want. The only line that needs to change is the one that defines the classifier inside the pipeline. It makes it very simple for us to try a large number of different models classes and determine which one is the best for the problem that we are working on. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using FunctionTransformer on the main dataset\n",
    "\n",
    "Now I am going to use FunctionTransformer on the primary budget data, before instantiating a multiple-datatype pipeline next.\n",
    "\n",
    "Above I used a custom function combine_text_columns to select and properly format text data for tokenization; it is loaded into the workspace and ready to be put to work in a function transformer!\n",
    "\n",
    "Concerning the numeric data, we can use NUMERIC_COLUMNS, preloaded as usual, to help design a subset-selecting lambda function.\n",
    "\n",
    "We're all finished with sample data. The original df is back in the workspace, ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Get the dummy encoding of the labels\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the columns that are features in the original df\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2, \n",
    "                                                               seed=123)\n",
    "\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x:x [NUMERIC_COLUMNS], validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will go forth and build a full pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding a model to the pipeline\n",
    "\n",
    "I am about to take everything we've learned so far and implement it in a Pipeline that works with the real, DrivenData budget line item data we've been exploring.\n",
    "\n",
    "The structure of the pipeline is exactly the same as earlier. \n",
    "\n",
    "The preprocessing step uses FeatureUnion to join the results of nested pipelines that each rely on FunctionTransformer to select multiple datatypes\n",
    "the model step stores the model object. \n",
    "\n",
    "We can then call familiar methods like .fit() and .score() on the Pipeline object pl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.35674223971019925\n"
     ]
    }
   ],
   "source": [
    "# Complete the pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've built the entire pipeline, we can easily start trying out different models by just modifying the 'clf' step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Trying a different class of model\n",
    "\n",
    "Now we're cruising. One of the great strengths of pipelines is how easy they make the process of testing different models.\n",
    "\n",
    "Until now, we've been using the model step ('clf', OneVsRestClassifier(LogisticRegression())) in our pipeline.\n",
    "\n",
    "But what if we want to try a different model? Do we need to build an entirely new pipeline? New nests? New FeatureUnions? Nope! We just have a simple one-line change, as you'll below. \n",
    "\n",
    "In particular, we'll swap out the logistic-regression model and replace it with a random forest classifier, which uses the statistics of an ensemble of decision trees to generate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.9041908687777154\n"
     ]
    }
   ],
   "source": [
    "# Import random forest classifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Edit model step in pipeline\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy improvement- amazing! All our work building the pipeline is paying off. It's now very simple to test different models!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Can We adjust the model or parameters to improve accuracy?\n",
    "\n",
    "You just saw a substantial improvement in accuracy by swapping out the model. Pipelines are amazing!\n",
    "\n",
    "Can we make it better? Try changing the parameter n_estimators of RandomForestClassifier(), whose default value is 10, to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.912922365873462\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Add model step to pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier(n_estimators=15))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, we're becoming a master! It's time to get serious and work with the log loss metric. We'll learn expert techniques next to take the model to the next level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Learning from the experts\n",
    "\n",
    "Here I will do some of the tricks done by a competition winner. Some of the tricks are about text processing, some are about statistical methods and some are about computational efficiency. The first trick the winner used was to tokenize on punctuation. By noticing that there are lots of characters like hyphens, periods, and underscores in the text we are working with, the winner picked out a better way of separating words than just spaces. The other trick was to use a range of n-gram in the model. By including unigrams and bigrams the winner was more likely to capture important information that appears as multiple tokens in the text, for example, “middle school”. One of the benefits of building our models on top of scikit-learn is that many of these common tools are implemented for us and are well-tested by a large community. To change our tokenization and add bi-grams we can change our call to CountVectorizer. We will pass the regular expression to only accept alpha-numeric characters in our tokens. We will also add the parameter ngram_range equals (1,2). This tells the CountVectorizer to include 1-grams and 2-grams in the vectorization. These changes to the call to CountVectorizer are the only changes we need to make in order to add these preprocessing steps. \n",
    "\n",
    "We can easily update the pipeline to include this change. Then we will fit our updated pipeline in the same way that we have been doing throughout this document, simply by calling the fit method on the pipeline object. Getting predictions from our model is also the same as it has been throughout. We use the predict_proba function to get our class probabilities. Because we have built our preprocessing into our pipeline, we don’t need to do any additional processing on our holdout data when we load it. The pipeline represents the entire prediction process from raw data to class probabilities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How many tokens?\n",
    "\n",
    "Recall from previous codes that how you tokenize text affects the n-gram statistics used in the model.\n",
    "\n",
    "Going forward, we'll use alpha-numeric sequences, and only alpha-numeric sequences, as tokens. Alpha-numeric tokens contain only letters a-z and numbers 0-9 (no other characters). In other words, we'll tokenize on punctuation to generate n-gram statistics.\n",
    "\n",
    "Here I will make sure to confirm how to tokenize on punctuation.\n",
    "\n",
    "Assuming we tokenize on punctuation, accepting only alpha-numeric sequences as tokens, how many tokens are in the following string from the main dataset?\n",
    "\n",
    "'PLANNING,RES,DEV,& EVAL'\n",
    "\n",
    "If you want, we've loaded this string into the workspace as SAMPLE_STRING, but we may not need it to answer the question.\n",
    "\n",
    "4, because , and & are not tokens\n",
    "\n",
    "Commas, \"&\", and whitespace are not alpha-numeric tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Deciding what's a word\n",
    "\n",
    "Before we build up to the winning pipeline, it will be useful to look a little deeper into how the text features will be processed.\n",
    "\n",
    "In this exercise, we will use CountVectorizer on the training data X_train (preloaded into the workspace) to see the effect of tokenization on punctuation.\n",
    "\n",
    "Remember, since CountVectorizer expects a vector, you'll need to use the preloaded function, combine_text_columns before fitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00a', '12', '1st', '2nd', '3rd', '4th', '5', '56', '5th', '6']\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate the CountVectorizer: text_features\n",
    "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit text_features to the text vector\n",
    "text_features.fit(text_vector)\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(text_features.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to start building the winning pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram range in scikit-learn\n",
    "\n",
    "I'll now insert a CountVectorizer instance into our pipeline for the main dataset, and compute multiple n-gram features to be used in the model.\n",
    "\n",
    "In order to look for ngram relationships at multiple scales, we will use the ngram_range parameter.\n",
    "\n",
    "Special functions: we'll notice a couple of new steps provided in the pipeline in this and many of the remaining codes. Specifically, the dim_red step following the vectorizer step , and the scale step preceeding the clf (classification) step.\n",
    "\n",
    "These have been added in order to account for the fact that we're using a reduced-size sample of the full dataset. To make sure the models perform as the expert competition winner intended, we have to apply a dimensionality reduction technique, which is what the dim_red step does, and we have to scale the features to lie between -1 and 1, which is what the scale step does.\n",
    "\n",
    "The dim_red step uses a scikit-learn function called SelectKBest(), applying something called the chi-squared test to select the K \"best\" features. The scale step uses a scikit-learn function called MaxAbsScaler() in order to squash the relevant features into the interval -1 to 1.\n",
    "\n",
    "We won't need to do anything extra with these functions here, just completing the vectorizing pipeline steps below. However, notice how easy it was to add more processing steps to our pipeline!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import other preprocessing modules\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Select 300 best features\n",
    "chi_k = 300\n",
    "\n",
    "# Import functional utilities\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Perform preprocessing\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll now add some additional tricks to make the pipeline even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Learning from the expert: a stats trick\n",
    "    \n",
    "Here I will use another tool that the winner used that is called interaction terms. We use bigrams to account for when words appear in a certain order. However, what if the terms are not next to each other? For example, consider “English Teacher for 2nd Grade” and “2nd Gramd – budget for English Teacher”. If we want to identify this line item as a staff position for an elementary school, it helps to know that both 2nd grade and English teacher appear. Interaction terms let us mathematically describe when tokens appear together. Scikit-learn provides us with a straightforward way of adding interaction terms. In scikit-learn this function is called PolynomialFeatures and we import it from the sklearn.preprocessing module. \n",
    "\n",
    "When we create a PloynomicalFeatures we tell it the degree of features to include. In our example we looked at multiplying 2 columns together to see if they co-occurred, so degree equals 2, however we could do 3, 4 or more as well. While this can sometimes improve the model results, adding larger degrees very quickly scales the number of features outside of what is computationally feasible. The interaction only equals True parameter tells PolynomicalFeatures that we don’t need to multiply a column by itself. We keep the include_bias = False. A bias term is an offset for a model. A bias term allows our model to account for situations where an x value of 0 should have a y value that is not 0. Adding interaction terms makes our X array grow exponentially. Because of computational concerns our CountVectorizer returns an object called a sparse matrix. PolynomicalFeatures object is not compatible with a sparse matrix. Sparseinteractions works with a sparse matrix. We only need to pass it the degree parameter for it to work in the same way. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Which models of the data include interaction terms?\n",
    "\n",
    "Recall from the video that interaction terms involve products of features.\n",
    "\n",
    "Suppose we have two features x and y, and we use models that process the features as follows:\n",
    "\n",
    "βx + βy + ββ\n",
    "βxy + βx + βy\n",
    "βx + βy + βx^2 + βy^2\n",
    "where β is a coefficient in your model (not a feature).\n",
    "\n",
    "Which expression(s) include interaction terms?\n",
    "\n",
    "Second expression \n",
    "\n",
    "An xy term is present, which represents interactions between features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Implementing interaction modeling in scikit-learn\n",
    "\n",
    "It's time to add interaction features to our model. The PolynomialFeatures object in scikit-learn does just that, but here we're going to use a custom interaction object, SparseInteractions. Interaction terms are a statistical tool that lets our model express what happens if two features appear together in the same row.\n",
    "\n",
    "SparseInteractions does the same thing as PolynomialFeatures, but it uses sparse matrices to do so. \n",
    "\n",
    "PolynomialFeatures and SparseInteractions both take the argument degree, which tells them what polynomial degree of interactions to compute.\n",
    "\n",
    "We're going to consider interaction terms of degree=2 in our pipeline. We will insert these steps after the preprocessing steps we've built out so far, but before the classifier steps.\n",
    "\n",
    "Pipelines with interaction terms take a while to train (since we're making n features into n-squared features!), so as long as we set it up right, we'll do the heavy lifting and tell what our score is!\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
    "        self.degree = degree\n",
    "        self.feature_name_separator = feature_name_separator\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not sparse.isspmatrix_csc(X):\n",
    "            X = sparse.csc_matrix(X)\n",
    "\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.orig_col_names = X.columns\n",
    "        else:\n",
    "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
    "\n",
    "        spi = self._create_sparse_interactions(X)\n",
    "        return spi\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "    def _create_sparse_interactions(self, X):\n",
    "        out_mat = []\n",
    "        self.feature_names = self.orig_col_names.tolist()\n",
    "\n",
    "        for sub_degree in range(2, self.degree + 1):\n",
    "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
    "                # add name for new column\n",
    "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
    "                self.feature_names.append(name)\n",
    "\n",
    "                # get column multiplications value\n",
    "                out = X[:, col_ixs[0]]\n",
    "                for j in col_ixs[1:]:\n",
    "                    out = out.multiply(X[:, j])\n",
    "\n",
    "                out_mat.append(out)\n",
    "\n",
    "        return sparse.hstack([X] + out_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),  \n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log loss improves here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Learning from the expert: the winning model\n",
    "    \n",
    "Now we need to balance adding new features with the computational cost of additional columns. For example, adding 3-grams or 4-grams is going to have an enormous increase in the size of the array. As the array grows in size we need more computational power to fit the model. \n",
    "\n",
    "The “hashing trick” is a way of limiting the size of the matrix that we create without sacrificing too much model accuracy. A hash function takes an input, in this case a token and outputs a hash value. The hash value may be an integer. We explicitly state how many possible outputs the hashing function may have. For example, we may say that we will only have 250 outputs of the hash function. The HashingVectorizer then maps every token to one of those 250 columns. Some columns will have multiple tokens that map to them. The original paper about the hashing function demonstrates that even if two tokens hash to the same value there is very little effect on model accuracy in real world problems. \n",
    "\n",
    "When working with big datasets as in this document, we want to do whatever we can to make our array of features smaller. Doing so is called “dimensionality reduction”. The hashing trick is particularly useful in cases like this where we have a very large amount of text data. Implementing the hashing trick is very simple in scikit-learn. Instead of using the CountVectorizer which creates the bag of words representation, we change to the HashingVectorizer. The parameters norm=None and non_negative = True, let us drop in the HashingVectorizer as a replacement for the CountVectorizer. I have worked through the three categories of tools that the winner put together to win the competition. The NLP trick was to use bigrams and tokenize on punctuation. The statistical trick was to add interaction terms to the model. The computational trick was to use a HashingVectorizer. \n",
    "\n",
    "I can now code all of these in a scikit-learn pipeline. The class of models that could be used: “deep convolutional neural network?, “extreme gradient boosted trees”?, “ensemble of local-expert elastic net regressions”?, The winning model used logistic regression. The competition wasn’t won by someone with a complex model but it was won by thinking carefully about how to create features for the model and then adding a couple of easily implemented tricks. It’s always worth it to see how far we can get with simpler methods. Now I will use all these tools to create the winning pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Why is hashing a useful trick?\n",
    "\n",
    "A hash function takes an input, in this case a token, and outputs a hash value. For example, the input may be a string and the hash value may be an integer.\n",
    "\n",
    "In fact, python dictionaries are hash tables!\n",
    "\n",
    "By explicitly stating how many possible outputs the hashing function may have, we limit the size of the objects that need to be processed. With these limits known, computation can be made more efficient and we can get results faster, even on large datasets.\n",
    "\n",
    "Using the above information, answer the following:\n",
    "\n",
    "Why is hashing a useful trick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hashing isn't useful unless you're working with numbers.\n",
    "\n",
    "Hashing can be used on any datatype.\n",
    "\n",
    "- Some problems are memory-bound and not easily parallelizable, but hashing parallelizes them.\n",
    "\n",
    "hashing does not necessarily have anything to do with parallelization.\n",
    "\n",
    "- Some problems are memory-bound and not easily parallelizable, and hashing enforces a fixed length computation instead of using a mutable datatype (like a dictionary).\n",
    "\n",
    "xCorrect.\n",
    "\n",
    "Enforcing a fixed length can speed up calculations drastically, especially on large datasets!\n",
    "\n",
    "- Hashing enforces a mutable length computation instead of using a fixed length datatype, like a dictionary.\n",
    "\n",
    "hashing enforced fixed length computation, and dictionaries are mutable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Implementing the hashing trick in scikit-learn\n",
    "\n",
    "Here we will check out the scikit-learn implementation of HashingVectorizer before adding it to our pipeline later.\n",
    "\n",
    "HashingVectorizer acts just like CountVectorizer in that it can accept token_pattern and ngram_range parameters. The important difference is that it creates hash values from the text, so that we get all the computational advantages of hashing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0  0.377964\n",
      "1  0.755929\n",
      "2  0.377964\n",
      "3  0.377964\n",
      "4  0.235702\n"
     ]
    }
   ],
   "source": [
    "# Import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Get text data: text_data\n",
    "text_data = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "\n",
    "# Instantiate the HashingVectorizer: hashing_vec\n",
    "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit and transform the Hashing Vectorizer\n",
    "hashed_text = hashing_vec.fit_transform(text_data)\n",
    "\n",
    "# Create DataFrame and print the head\n",
    "hashed_df = pd.DataFrame(hashed_text.data)\n",
    "print(hashed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some text is hashed to the same value, this doesn't neccessarily hurt performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Building the winning model\n",
    "\n",
    "This is where all of our hard work pays off. It's time to build the model that won DrivenData's competition.\n",
    "\n",
    "We've constructed a robust, powerful pipeline capable of processing training and testing data. Now that we understand the data and know all of the tools we need, we can essentially solve the whole problem in a relatively small number of lines of code. Wow!\n",
    "\n",
    "All we need to do is add the HashingVectorizer step to the pipeline to replace the CountVectorizer step.\n",
    "\n",
    "The parameters non_negative=True, norm=None, and binary=False make the HashingVectorizer perform similarly to the default settings on the CountVectorizer so we can just replace one with the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.multilabel import multilabel_sample_dataframe, multilabel_train_test_split\n",
    "from features.SparseInteractions import SparseInteractions\n",
    "from models.metrics import multi_multi_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "log_loss_scorer = make_scorer(multi_multi_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import the hashing vectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Instantiate the winning model pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                     norm=None, binary=False,\n",
    "                                                     ngram_range=(1,2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the performance is about the same as CountVectorizer, but this is expected since the HashingVectorizer should work the same as the CountVectorizer. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import the hashing vectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Instantiate the winning model pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                     alternate_sign=False, norm=None, binary=False,\n",
    "                                                     ngram_range=(1,2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline to our training data\n",
    "pl.fit(X_train, y_train.values)\n",
    "\n",
    "# print the score of our trained pipeline on our test set\n",
    "print(\"Logloss score of trained pipeline: \", log_loss_scorer(pl, X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What tactics got the winner the best score?\n",
    "\n",
    "Now we've implemented the winning model from start to finish. \n",
    "\n",
    "Let's take a moment to reflect on why this model did so well. What tactics got the winner the best score?\n",
    "\n",
    "The winner used a 500 layer deep convolutional neural network to master the budget data.\n",
    "\n",
    "The winner used an ensemble of many models for classification, taking the best results as predictions.\n",
    "\n",
    "The winner used skillful NLP, efficient computation, and simple but powerful stats tricks to master the budget data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times simpler is better, and understanding the problem in depth leads to simpler solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks a lot for your attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
